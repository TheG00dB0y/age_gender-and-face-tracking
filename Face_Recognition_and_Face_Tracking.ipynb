{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Recognition and Face Tracking.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmje9dW7Anqv"
      },
      "source": [
        "!rm \"/content/Face_ID/facepics\" -r\n",
        "!rm \"/content/Face_ID/outputs\" -r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_0-7y6yf-ou"
      },
      "source": [
        "!git clone https://github.com/TheG00dB0y/Face_ID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D21MWF_YgGbd"
      },
      "source": [
        "%cd Face_ID/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGio41WXHvod"
      },
      "source": [
        "!cp \"/content/drive/My Drive/yoloface/cfg\" \"cfg\" -r\n",
        "!cp \"/content/drive/My Drive/yoloface/model-weights\" \"model-weights\" -r\n",
        "!cp \"/content/drive/My Drive/yoloface/samples\" \"samples\" -r\n",
        "!cp \"/content/drive/My Drive/yoloface/utils.py\" \"utils.py\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtUUHGKlX3rw"
      },
      "source": [
        "!mkdir \"facepics\"\n",
        "!mkdir \"outputs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwe3CDayhAz2"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "#face_rec\n",
        "import tensorflow as tf\n",
        "from fr_utils import *\n",
        "from inception_blocks_v2 import *\n",
        "from utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkdJJxM-YDnL"
      },
      "source": [
        "def iou(box1, box2):\n",
        "\n",
        "    (box1_x1, box1_y1, box1_x2, box1_y2) = box1\n",
        "    (box2_x1, box2_y1, box2_x2, box2_y2) = box2\n",
        "    \n",
        "    # Calculate the (yi1, xi1, yi2, xi2) coordinates of the intersection \n",
        "    xi1 = max(box1_x1,box2_x1)\n",
        "    yi1 = max(box1_y1,box2_y1)\n",
        "    xi2 = min(box1_x2,box2_x2)\n",
        "    yi2 = min(box1_y2,box2_y2)\n",
        "    inter_width = xi2-xi1\n",
        "    inter_height = yi2-yi1\n",
        "    #intersection area\n",
        "    inter_area = max(inter_width,0) * max(inter_height,0)\n",
        " \n",
        "\n",
        "    box1_area = (box1_x2-box1_x1) * (box1_y2-box1_y1)\n",
        "    box2_area = (box2_x2-box2_x1) * (box2_y2-box2_y1)\n",
        "    #union\n",
        "    union_area = (box1_area + box2_area) - inter_area\n",
        "\n",
        "    iou = inter_area / union_area\n",
        "\n",
        "    return iou\n",
        "class Feather_Track(object):\n",
        "  def __init__(self, max_age=1, iou_thresh =0.4):\n",
        "\n",
        "    #Age denotes the expiring of a track\n",
        "    self.max_age = max_age\n",
        "    self.iou_thresh = iou_thresh\n",
        "    self.p_track = []\n",
        "    self.frame_count = 0\n",
        "    self.id = 0\n",
        "    \n",
        "  def update(self, dets=np.empty((0, 5))):\n",
        "    # print(dets)\n",
        "    self.frame_count += 1\n",
        "    # self.p_track[:]['age'] += 1\n",
        "    updated_trk = []\n",
        "    # if self.id==0 and dets.size != 0:\n",
        "    if not self.p_track and dets.size != 0:\n",
        "      #Initialise first detections as Previous Trackers & if not first add with previous id(When p_track becomes empty)\n",
        "      self.p_track = [{'bbox':list(det),'id':k + self.id,'age':0,'frame_count':1} for k,det in enumerate(dets,1)]\n",
        "      self.id = self.p_track[-1]['id']\n",
        "    elif self.p_track and dets.size != 0:\n",
        "      # print(\"####\")\n",
        "      # print(self.p_track)\n",
        "      new_p = self.p_track\n",
        "      for det in dets:\n",
        "        try:\n",
        "          print(det)\n",
        "          print(new_p[0]['bbox'])\n",
        "          best_match_trk = max(new_p,key=lambda x:iou(det,x['bbox']))\n",
        "        except ValueError:\n",
        "          print(\"################################### VALUE ERROR ####################################################\\n\"+\"det: \",end=\" \")\n",
        "          print(det)\n",
        "          print(\"prev trk: \",end=\" \")\n",
        "          print(self.p_track)\n",
        "          continue\n",
        "        # print(iou(det,best_match_trk['bbox']))\n",
        "        if iou(det,best_match_trk['bbox']) >= self.iou_thresh :\n",
        "          best_match_trk['bbox'] = det\n",
        "          best_match_trk['frame_count'] += 1\n",
        "          # print(best_match_trk)\n",
        "          #reset age\n",
        "          best_match_trk['age'] = 0\n",
        "          updated_trk.append(best_match_trk)\n",
        "        else:\n",
        "          #New non matched detection is assigned to new track\n",
        "          self.id += 1\n",
        "          new_trk = {'bbox':det,'id':self.id,'age':0,'frame_count':1}\n",
        "          updated_trk.append(new_trk)\n",
        "          if best_match_trk['age'] <= self.max_age:\n",
        "            best_match_trk['age'] += 1\n",
        "            updated_trk.append(best_match_trk)\n",
        "        #removing the used best_match_trk from p_track\n",
        "        # del dets[dets.index(best_match)]\n",
        "        # print(self.p_track[self.p_track.index(best_match_trk)])\n",
        "        # print(\"before:\")\n",
        "        # print(self.p_track)\n",
        "        # print(best_match_trk)\n",
        "        # print(self.p_track.index(best_match_trk))\n",
        "        # del self.p_track[self.p_track.index(best_match_trk)]\n",
        "        # self.p_track.remove(best_match_trk)\n",
        "        self.p_track = list(filter(lambda x: x['id'] != best_match_trk['id'] and x['age'] < self.max_age, self.p_track)) \n",
        "        # print(\"after:\")\n",
        "        # print(self.p_track)        \n",
        "      #increase age of previous tracks\n",
        "      for trk in self.p_track:\n",
        "        trk['age'] += 1\n",
        "      self.p_track = self.p_track + updated_trk\n",
        "    elif self.p_track and dets.size == 0:\n",
        "      new_p = []\n",
        "      for trk in self.p_track:\n",
        "        trk['age'] += 1\n",
        "        if(trk['age'] < self.max_age):\n",
        "          new_p.append(trk)\n",
        "      self.p_track = new_p\n",
        "      return []\n",
        "        \n",
        "      \n",
        "    # print(\"#######\",end = \" \")\n",
        "    # print(self.p_track,end=\"#######\")\n",
        "    return self.p_track\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqf_aiPbb0rl",
        "outputId": "00272455-3e62-4bd9-f9cc-a4d5f87a86f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "model_cfg = './cfg/yolov3-face.cfg'\n",
        "model_weights = './model-weights/yolov3-wider_16000.weights'\n",
        "# image = './samples/outside_000001.jpg'\n",
        "video = '/content/Face_ID/samples/New video.mp4'\n",
        "# output_dir = '/content/drive/My Drive/yoloface/outputs/'\n",
        "output_dir = './outputs/'\n",
        "\n",
        "IOU_THRESH = 0.35\n",
        "MAX_AGE = 3\n",
        "#Trackers(Here faces) with given frame count value is chosen for age and gender processing\n",
        "FRAME_COUNT_THRESH = 6\n",
        "# DETECT_INTERVAL = 2\n",
        "FACE_HEIGHT = 50\n",
        "FACE_WIDTH = 50\n",
        "# print the arguments\n",
        "print('----- info -----')\n",
        "print('[i] The config file: ',model_cfg)\n",
        "print('[i] The weights of model file: ',model_weights)\n",
        "# print('[i] Path to image file: ',image)\n",
        "print('[i] Path to video file: ',video)\n",
        "print('[i] IOU Threshold value: ',IOU_THRESH)\n",
        "print('[i] Max age value: ',MAX_AGE)\n",
        "print('[i] Frame count threshold value: ',FRAME_COUNT_THRESH)\n",
        "# print('[i] Detect interval: ',DETECT_INTERVAL)\n",
        "print('[i] Minimum face height required: ',FACE_HEIGHT)\n",
        "print('[i] Minimum face width required: ',FACE_WIDTH)\n",
        "\n",
        "\n",
        "#Read config net\n",
        "net = cv2.dnn.readNetFromDarknet(model_cfg,model_weights)\n",
        "print('###########################################################\\n')\n",
        "\n",
        "\n",
        "#VIDEO\n",
        "cap = cv2.VideoCapture(video)\n",
        "output_file = video[:-4].rsplit('/')[-1] + '_keras.avi'\n",
        "output_file = output_dir + output_file\n",
        "\n",
        "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
        "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
        "\n",
        "video_writer = cv2.VideoWriter(output_file,cv2.VideoWriter_fourcc(*'XVID'),cap.get(cv2.CAP_PROP_FPS), (\n",
        "                                          round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "                                          round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
        "\n",
        "\n",
        "# print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "# print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "print(cap.get(cv2.CAP_PROP_FPS))\n",
        "#create instance of the multi object tracker\n",
        "mot = Feather_Track(MAX_AGE,IOU_THRESH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- info -----\n",
            "[i] The config file:  ./cfg/yolov3-face.cfg\n",
            "[i] The weights of model file:  ./model-weights/yolov3-wider_16000.weights\n",
            "[i] Path to video file:  /content/Face_ID/samples/New video.mp4\n",
            "[i] IOU Threshold value:  0.35\n",
            "[i] Max age value:  3\n",
            "[i] Frame count threshold value:  6\n",
            "[i] Minimum face height required:  50\n",
            "[i] Minimum face width required:  50\n",
            "###########################################################\n",
            "\n",
            "30.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgHQ9rVgeC5F"
      },
      "source": [
        "# Create a new model instance\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "FRmodel = faceRecoModel(input_shape=(3, 96, 96))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvU7STTqiMQi"
      },
      "source": [
        "# Restore the weights\n",
        "FRmodel.load_weights('./weight_file')\n",
        "\n",
        "def img_encoding(img1, model):\n",
        "    img = img1[...,::-1]\n",
        "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
        "    x_train = np.array([img])\n",
        "    embedding = model.predict_on_batch(x_train)\n",
        "    return embedding\n",
        "\n",
        "def who_is_it(img, database, model):\n",
        "        \n",
        "    encoding = img_encoding(img,model)\n",
        "    min_dist = 100\n",
        "    for (name, db_enc) in database.items():\n",
        "      # Compute L2 distance between the target \"encoding\" and the current db_enc from the database. (≈ 1 line)\n",
        "      dist = np.linalg.norm(db_enc-encoding)\n",
        "      # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n",
        "      if dist<min_dist:\n",
        "        min_dist = dist\n",
        "        identity = name\n",
        "\n",
        "    if min_dist > 0.7:\n",
        "        # print(\"Not in the database.\")\n",
        "        return ''\n",
        "    else:\n",
        "        # print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
        "        return identity\n",
        "        \n",
        "    return identity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCZNTfE56nAi"
      },
      "source": [
        "database = {}\n",
        "database[\"Athul\"] = img_to_encoding(\"/content/Face_ID/facepics/1.jpg\", FRmodel)\n",
        "database[\"Alveena\"] = img_to_encoding(\"/content/Face_ID/facepics/2.jpg\", FRmodel)\n",
        "database[\"Ajay\"] = img_to_encoding(\"/content/Face_ID/facepics/3.jpg\", FRmodel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoF1N_Jn3tFT"
      },
      "source": [
        "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
        "               font_scale=1.0, thickness=3):\n",
        "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
        "    x, y = point\n",
        "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
        "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9ogZTM86EmN"
      },
      "source": [
        "margin = 0.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXfuO5yoea3C"
      },
      "source": [
        "#Frame counter for detect interval\n",
        "# c=0\n",
        "%time\n",
        "while True:\n",
        "  \n",
        "  has_frame, frame = cap.read()\n",
        "  if not has_frame:\n",
        "      print('[i] ==> Done processing!!!')\n",
        "      #print('[i] ==> Output file is stored at', os.path.join(args.output_dir, output_file))\n",
        "      break\n",
        "  # frame = cv2.resize(frame, (0, 0), fx=0.7, fy=0.7)\n",
        "  # Resize, Convert BGR to HSV\n",
        "  # if ((IMG_HEIGHT, IMG_WIDTH) != frame.shape[0:2]):\n",
        "  #     frame = cv2.resize(frame, dsize=(IMG_WIDTH, IMG_HEIGHT), fx=0, fy=0)\n",
        "  # else:\n",
        "  #     frame = frame\n",
        "  # if(c % DETECT_INTERVAL==0):\n",
        "  new_frame = frame.copy()\n",
        "  img_h, img_w, _ = np.shape(frame)\n",
        "\n",
        "  blob = cv2.dnn.blobFromImage(frame, 1 / 255, (IMG_WIDTH, IMG_HEIGHT),[0, 0, 0], 1, crop=False)\n",
        "  net.setInput(blob)\n",
        "  # Runs the forward pass to get output of the output layers\n",
        "  outs = net.forward(get_outputs_names(net))\n",
        "  # Remove the bounding boxes with low confidence\n",
        "  faces = post_process(frame, outs,CONF_THRESHOLD, NMS_THRESHOLD)\n",
        "  faces = np.array(faces) \n",
        "  \n",
        "  # faces_resized = []\n",
        "\n",
        "  if(faces.shape[0]>0):\n",
        "    final_faces = faces[:,:4]\n",
        "    \n",
        "  else:\n",
        "    final_faces = faces\n",
        "  trackers = mot.update(final_faces)\n",
        "  print(\"Final faces:\",end=\" \")\n",
        "  print(final_faces)\n",
        "  print(\"$$$$$$$$$$$$$ \") \n",
        "  print(trackers)\n",
        "  print(\"&&&&&&&&&&&&&&&&&&\")\n",
        "  print(\"#\",end=\"\")\n",
        "  # faces_resized = np.empty((len(trackers), img_size, img_size, 3))\n",
        "\n",
        "  for i,trk in enumerate(trackers):\n",
        "    box = np.array(trk['bbox'])\n",
        "    id = int(trk['id'])\n",
        "    frame_count = trk['frame_count']\n",
        "    d = box.astype(np.int32)\n",
        "    (startX, startY, endX, endY) = (d[0],d[1],d[2],d[3])\n",
        "    face_h = endY - startY\n",
        "    face_w = endX - startX\n",
        "\n",
        "    x1, y1, x2, y2, w, h = d[0], d[1], d[2] + 1, d[3] + 1, face_w, face_h\n",
        "    xw1 = max(int(x1 - margin * w), 0)\n",
        "    yw1 = max(int(y1 - margin * h), 0)\n",
        "    xw2 = min(int(x2 + margin * w), img_w - 1)\n",
        "    yw2 = min(int(y2 + margin * h), img_h - 1)\n",
        "    # cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "    cv2.rectangle(frame, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\n",
        "    # faces[i, :, :, :] = cv2.resize(img[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # if(frame_count==FRAME_COUNT_THRESH and (face_h > FACE_HEIGHT and face_w > FACE_WIDTH)):\n",
        "    # crop_face = new_frame[startY:endY, startX:endX]\n",
        "    # crop_face_shape = new_frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
        "    # crop_face = cv2.resize(new_frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (96, 96))\n",
        "\n",
        "    \n",
        "    try:\n",
        "      crop_face = cv2.resize( new_frame[startY:endY, startX:endX], (96, 96))\n",
        "      print(crop_face.shape)\n",
        "    except:\n",
        "      continue\n",
        "    # faces_resized[i,:,:,:] = crop_face\n",
        "\n",
        "    # if(frame_count == FRAME_COUNT_THRESH):  \n",
        "    #   try:\n",
        "    #     cv2.imwrite(\"{0}/{1}.jpg\".format(\"facepics\",id),crop_face)\n",
        "    #     # cv2_imshow(crop_face)\n",
        "    #   except:\n",
        "    #     print(\"#########################Image error####################\")\n",
        "    #     continue\n",
        "    #identity = who_is_it(crop_face, database, FRmodel)   \n",
        "    identity = \"\" \n",
        "    print(identity)\n",
        "    label = \"{}, {}\".format(id,identity)\n",
        "    draw_label(frame, (d[0], d[1]), label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # cv2_imshow(frame)\n",
        "  video_writer.write(frame.astype(np.uint8))\n",
        "  # c+=1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhXshRFJgo0Q"
      },
      "source": [
        "cap.release()\n",
        "video_writer.release()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}