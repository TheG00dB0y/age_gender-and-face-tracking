{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Age Gender with Face Tracking.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmfWZ1OHstTL"
      },
      "source": [
        "# !ls\n",
        "%cd /content/drive/My Drive/yoloface\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from utils import *\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def iou(box1, box2):\n",
        "    (box1_x1, box1_y1, box1_x2, box1_y2) = box1\n",
        "    (box2_x1, box2_y1, box2_x2, box2_y2) = box2\n",
        "    \n",
        "    # Calculate the (yi1, xi1, yi2, xi2) \n",
        "    xi1 = max(box1_x1,box2_x1)\n",
        "    yi1 = max(box1_y1,box2_y1)\n",
        "    xi2 = min(box1_x2,box2_x2)\n",
        "    yi2 = min(box1_y2,box2_y2)\n",
        "    inter_width = xi2-xi1\n",
        "    inter_height = yi2-yi1\n",
        "    #inter area\n",
        "    inter_area = max(inter_width,0) * max(inter_height,0)\n",
        "\n",
        "    box1_area = (box1_x2-box1_x1) * (box1_y2-box1_y1)\n",
        "    box2_area = (box2_x2-box2_x1) * (box2_y2-box2_y1)\n",
        "    #union area\n",
        "    union_area = (box1_area + box2_area) - inter_area\n",
        "    \n",
        "    iou = inter_area / union_area\n",
        "    return iou\n",
        "\n",
        "class Feather_Track(object):\n",
        "  def __init__(self, max_age=1, iou_thresh =0.4):\n",
        "    #Age denotes the expiring of a track\n",
        "    self.max_age = max_age\n",
        "    self.iou_thresh = iou_thresh\n",
        "    self.p_track = []\n",
        "    self.frame_count = 0\n",
        "    self.id = 0\n",
        "    \n",
        "  def update(self, dets=np.empty((0, 5))):\n",
        "    # print(dets)\n",
        "    self.frame_count += 1\n",
        "    # self.p_track[:]['age'] += 1\n",
        "    updated_trk = []\n",
        "    # if self.id==0 and dets.size != 0:\n",
        "    if not self.p_track and dets.size != 0:\n",
        "      #Initialise first detections as Previous Trackers\n",
        "      self.p_track = [{'bbox':list(det),'id':k,'age':0,'frame_count':1} for k,det in enumerate(dets,1)]\n",
        "      self.id = self.p_track[-1]['id']\n",
        "    elif self.p_track and dets.size != 0:\n",
        "      # print(\"####\")\n",
        "      # print(self.p_track)\n",
        "      new_p = self.p_track\n",
        "      for det in dets:\n",
        "        try:\n",
        "          best_match_trk = max(new_p,key=lambda x:iou(det,x['bbox']))\n",
        "        except ValueError:\n",
        "          print(\"################################### VALUE ERROR ####################################################\\n\"+\"det: \",end=\" \")\n",
        "          print(det)\n",
        "          print(\"prev trk: \",end=\" \")\n",
        "          print(self.p_track)\n",
        "          continue\n",
        "        # print(iou(det,best_match_trk['bbox']))\n",
        "        if iou(det,best_match_trk['bbox']) >= self.iou_thresh :\n",
        "          best_match_trk['bbox'] = det\n",
        "          best_match_trk['frame_count'] += 1\n",
        "          # print(best_match_trk)\n",
        "          updated_trk.append(best_match_trk)\n",
        "        else:\n",
        "          #New non matched detection is assigned to new track\n",
        "          self.id += 1\n",
        "          new_trk = {'bbox':det,'id':self.id,'age':0,'frame_count':1}\n",
        "          updated_trk.append(new_trk)\n",
        "          if best_match_trk['age'] <= self.max_age:\n",
        "            best_match_trk['age'] += 1\n",
        "            updated_trk.append(best_match_trk)\n",
        "        #removing the used best_match_trk from p_track\n",
        "        # del dets[dets.index(best_match)]\n",
        "        # print(self.p_track[self.p_track.index(best_match_trk)])\n",
        "        # print(\"before:\")\n",
        "        # print(self.p_track)\n",
        "        # print(best_match_trk)\n",
        "        # print(self.p_track.index(best_match_trk))\n",
        "        # del self.p_track[self.p_track.index(best_match_trk)]\n",
        "        # self.p_track.remove(best_match_trk)\n",
        "        self.p_track = list(filter(lambda x: x['id'] != best_match_trk['id'] and x['age'] < self.max_age, self.p_track)) \n",
        "        # print(\"after:\")\n",
        "        # print(self.p_track)        \n",
        "      #increase age of previous tracks\n",
        "      for trk in self.p_track:\n",
        "        trk['age'] += 1\n",
        "      self.p_track = self.p_track + updated_trk\n",
        "    elif self.p_track and dets.size == 0:\n",
        "      new_p = []\n",
        "      for trk in self.p_track:\n",
        "        trk['age'] += 1\n",
        "        if(trk['age'] < self.max_age):\n",
        "          new_p.append(trk)\n",
        "      self.p_track = new_p\n",
        "      return []\n",
        "        \n",
        "      \n",
        "    # print(\"#######\",end = \" \")\n",
        "    # print(self.p_track,end=\"#######\")\n",
        "    return self.p_track\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsuUcJA_vS0J",
        "outputId": "b4975776-a759-4b0a-974e-844cbce5c692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "model_cfg = './cfg/yolov3-face.cfg'\n",
        "model_weights = './model-weights/yolov3-wider_16000.weights'\n",
        "# image = './samples/outside_000001.jpg'\n",
        "video = './samples/trance.mp4'\n",
        "output_dir = '/content/drive/My Drive/yoloface/outputs/'\n",
        "IOU_THRESH = 0.35\n",
        "MAX_AGE = 1\n",
        "#Trackers(Here faces) with given frame count value is chosen for age and gender processing\n",
        "FRAME_COUNT_THRESH = 10\n",
        "# DETECT_INTERVAL = 2\n",
        "FACE_HEIGHT = 70\n",
        "FACE_WIDTH = 70\n",
        "# print the arguments\n",
        "print('----- info -----')\n",
        "print('[i] The config file: ',model_cfg)\n",
        "print('[i] The weights of model file: ',model_weights)\n",
        "# print('[i] Path to image file: ',image)\n",
        "print('[i] Path to video file: ',video)\n",
        "print('[i] IOU Threshold value: ',IOU_THRESH)\n",
        "print('[i] Max age value: ',MAX_AGE)\n",
        "print('[i] Frame count threshold value: ',FRAME_COUNT_THRESH)\n",
        "# print('[i] Detect interval: ',DETECT_INTERVAL)\n",
        "print('[i] Minimum face height required: ',FACE_HEIGHT)\n",
        "print('[i] Minimum face width required: ',FACE_WIDTH)\n",
        "\n",
        "\n",
        "#Read config net\n",
        "net = cv2.dnn.readNetFromDarknet(model_cfg,model_weights)\n",
        "print('###########################################################\\n')\n",
        "\n",
        "\n",
        "#VIDEO\n",
        "cap = cv2.VideoCapture(video)\n",
        "output_file = video[:-4].rsplit('/')[-1] + '_yoloface.avi'\n",
        "output_file = output_dir + output_file\n",
        "video_writer = cv2.VideoWriter(output_file,cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'),cap.get(cv2.CAP_PROP_FPS), (\n",
        "                                          round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "                                          round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
        "print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "print(cap.get(cv2.CAP_PROP_FPS))\n",
        "#create instance of the multi object tracker\n",
        "mot = Feather_Track(MAX_AGE,IOU_THRESH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- info -----\n",
            "[i] The config file:  ./cfg/yolov3-face.cfg\n",
            "[i] The weights of model file:  ./model-weights/yolov3-wider_16000.weights\n",
            "[i] Path to video file:  ./samples/trance.mp4\n",
            "[i] IOU Threshold value:  0.35\n",
            "[i] Max age value:  1\n",
            "[i] Frame count threshold value:  10\n",
            "[i] Minimum face height required:  70\n",
            "[i] Minimum face width required:  70\n",
            "###########################################################\n",
            "\n",
            "1280.0\n",
            "544.0\n",
            "24.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ajnq_3yE2af"
      },
      "source": [
        "ageProto=\"./age_gender_net/deploy_age.prototxt\"\n",
        "ageModel=\"./age_gender_net/age_net.caffemodel\"\n",
        "genderProto=\"./age_gender_net/deploy_gender.prototxt\"\n",
        "genderModel=\"./age_gender_net/gender_net.caffemodel\"\n",
        "MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)\n",
        "ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
        "genderList=['Male','Female']\n",
        "id_dict = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb7dGPz0T2G6"
      },
      "source": [
        "#Frame counter for detect interval\n",
        "c=0\n",
        "while True:\n",
        "  has_frame, frame = cap.read()\n",
        "  if not has_frame:\n",
        "      print('[i] ==> Done processing!!!')\n",
        "      #print('[i] ==> Output file is stored at', os.path.join(args.output_dir, output_file))\n",
        "      break\n",
        "  # Resize, Convert BGR to HSV\n",
        "  # if ((IMG_HEIGHT, IMG_WIDTH) != frame.shape[0:2]):\n",
        "  #     frame = cv2.resize(frame, dsize=(IMG_WIDTH, IMG_HEIGHT), fx=0, fy=0)\n",
        "  # else:\n",
        "  #     frame = frame\n",
        "  # if(c % DETECT_INTERVAL==0):\n",
        "  new_frame = frame.copy()\n",
        "  blob = cv2.dnn.blobFromImage(frame, 1 / 255, (IMG_WIDTH, IMG_HEIGHT),[0, 0, 0], 1, crop=False)\n",
        "  net.setInput(blob)\n",
        "  # Runs the forward pass to get output of the output layers\n",
        "  outs = net.forward(get_outputs_names(net))\n",
        "  # Remove the bounding boxes with low confidence\n",
        "  faces = post_process(frame, outs,CONF_THRESHOLD, NMS_THRESHOLD)\n",
        "  faces = np.array(faces) \n",
        "  if(faces.shape[0]>0):\n",
        "    final_faces = faces[:,:4]\n",
        "  else:\n",
        "    final_faces = faces\n",
        "  trackers = mot.update(final_faces)\n",
        "  # print(\"LOG: Final faces = \",end=\" \")\n",
        "  # print(final_faces)\n",
        "  # print(\"$$$$$$$$$$$$$ \") \n",
        "  # print(trackers)\n",
        "  # print(\"&&&&&&&&&&&&&&&&&&\")\n",
        "  print(\"#\",end=\"\")\n",
        "\n",
        "  for trk in trackers:\n",
        "    box = np.array(trk['bbox'])\n",
        "    id = int(trk['id'])\n",
        "    frame_count = trk['frame_count']\n",
        "    d = box.astype(np.int32)\n",
        "    (startX, startY, endX, endY) = (d[0],d[1],d[2],d[3])\n",
        "    face_h = endY - startY\n",
        "    face_w = endX - startX\n",
        "    if(frame_count==FRAME_COUNT_THRESH and (face_h > FACE_HEIGHT and face_w > FACE_WIDTH)):\n",
        "      crop_face = new_frame[startY:endY, startX:endX]\n",
        "      print(crop_face.shape)\n",
        "\n",
        "      ageNet=cv2.dnn.readNet(ageModel,ageProto)\n",
        "      genderNet=cv2.dnn.readNet(genderModel,genderProto)\n",
        "      blob=cv2.dnn.blobFromImage(crop_face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)\n",
        "      genderNet.setInput(blob)\n",
        "      genderPreds=genderNet.forward()\n",
        "      gender=genderList[genderPreds[0].argmax()]\n",
        "      # print(\"gender:\",gender)\n",
        "      ageNet.setInput(blob)\n",
        "      agePreds=ageNet.forward()\n",
        "      age=ageList[agePreds[0].argmax()]\n",
        "      # print(\"Age:\",age)\n",
        "      id_dict[id] = [gender,age]\n",
        "       \n",
        "      try:\n",
        "        cv2.imwrite(\"{0}/{1}_{2}_{3}.jpg\".format(\"facepics\",id,gender,age),crop_face)\n",
        "      except:\n",
        "        print(\"#########################Image error####################\")\n",
        "        continue\n",
        "\n",
        "    if(id in id_dict.keys()):\n",
        "       cv2.putText(frame, 'ID:%d %s' %(id,gender), (d[0] - 10, d[1] - 10),cv2.FONT_HERSHEY_SIMPLEX,0.5,COLOR_BLUE, 2)\n",
        "    else:\n",
        "      cv2.putText(frame, 'ID:%d' %id, (d[0] - 10, d[1] - 10),cv2.FONT_HERSHEY_SIMPLEX,0.5,COLOR_BLUE, 2)\n",
        "    \n",
        "    cv2.rectangle(frame,(d[0],d[1]),(d[2],d[3]),COLOR_BLUE, 3)\n",
        "    #Frame count display\n",
        "    text = '{}: {}'.format(\"Frame Count: \",c)\n",
        "    cv2.putText(frame, text, (10,20),cv2.FONT_HERSHEY_SIMPLEX, 0.7, COLOR_RED, 2)\n",
        "\n",
        "  c+=1\n",
        "  video_writer.write(frame.astype(np.uint8))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "604xHQWTiCw0"
      },
      "source": [
        "cap.release()\n",
        "video_writer.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsmBlxgQuiHT"
      },
      "source": [
        "############################################################################\n",
        "#END"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}