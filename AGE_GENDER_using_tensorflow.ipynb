{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AGE GENDER using tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqUdpFqQ_fzI"
      },
      "source": [
        "!git clone https://github.com/yu4u/age-gender-estimation.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cypoSkK8Duug"
      },
      "source": [
        "%cd age-gender-estimation/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtUUHGKlX3rw"
      },
      "source": [
        "!mkdir \"facepics\"\n",
        "!mkdir \"outputs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGio41WXHvod"
      },
      "source": [
        "!cp \"/content/drive/My Drive/yoloface/cfg\" \"cfg\" -r\n",
        "!cp \"/content/drive/My Drive/yoloface/model-weights\" \"model-weights\" -r\n",
        "!cp \"/content/drive/My Drive/yoloface/samples\" \"samples\" -r\n",
        "!cp \"/content/drive/My Drive/yoloface/utils.py\" \"utils.py\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNZGOmXqAfQT"
      },
      "source": [
        "# !python demo.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOYzFdTxCImc"
      },
      "source": [
        "from pathlib import Path\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "from contextlib import contextmanager\n",
        "from wide_resnet import WideResNet\n",
        "from keras.utils.data_utils import get_file\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
        "               font_scale=0.8, thickness=1):\n",
        "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
        "    x, y = point\n",
        "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
        "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def video_capture(*args, **kwargs):\n",
        "    cap = cv2.VideoCapture(*args, **kwargs)\n",
        "    try:\n",
        "        yield cap\n",
        "    finally:\n",
        "        cap.release()\n",
        "\n",
        "\n",
        "def yield_images():\n",
        "    # capture video\n",
        "    with video_capture(\"/content/shakira.mp4\") as cap:\n",
        "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
        "\n",
        "        while True:\n",
        "            # get video frame\n",
        "            ret, img = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                raise RuntimeError(\"Failed to capture image\")\n",
        "\n",
        "            yield img\n",
        "\n",
        "depth = 16\n",
        "k = 8\n",
        "weight_file = \"/content/age-gender-estimation/pretrained_models/weights.28-3.73.hdf5\"\n",
        "margin = 0.4\n",
        "image_dir = None\n",
        "\n",
        "if not weight_file:\n",
        "    weight_file = get_file(\"weights.28-3.73.hdf5\", pretrained_model, cache_subdir=\"pretrained_models\",\n",
        "                            file_hash=modhash, cache_dir=str(Path(__file__).resolve().parent))\n",
        "\n",
        "# for face detection\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "# load model and weights\n",
        "img_size = 64\n",
        "model = WideResNet(img_size, depth=depth, k=k)()\n",
        "model.load_weights(weight_file)\n",
        "\n",
        "image_generator = yield_images_from_dir(image_dir) if image_dir else yield_images()\n",
        "\n",
        "for img in image_generator:\n",
        "    input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img_h, img_w, _ = np.shape(input_img)\n",
        "\n",
        "    # detect faces using dlib detector\n",
        "    detected = detector(input_img, 1)\n",
        "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
        "\n",
        "    if len(detected) > 0:\n",
        "        for i, d in enumerate(detected):\n",
        "            x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
        "            xw1 = max(int(x1 - margin * w), 0)\n",
        "            yw1 = max(int(y1 - margin * h), 0)\n",
        "            xw2 = min(int(x2 + margin * w), img_w - 1)\n",
        "            yw2 = min(int(y2 + margin * h), img_h - 1)\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "            # cv2.rectangle(img, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\n",
        "            faces[i, :, :, :] = cv2.resize(img[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
        "\n",
        "        # predict ages and genders of the detected faces\n",
        "        results = model.predict(faces)\n",
        "        predicted_genders = results[0]\n",
        "        ages = np.arange(0, 101).reshape(101, 1)\n",
        "        predicted_ages = results[1].dot(ages).flatten()\n",
        "\n",
        "        # draw results\n",
        "        for i, d in enumerate(detected):\n",
        "            label = \"{}, {}\".format(int(predicted_ages[i]),\n",
        "                                    \"M\" if predicted_genders[i][0] < 0.5 else \"F\")\n",
        "            draw_label(img, (d.left(), d.top()), label)\n",
        "\n",
        "    # cv2.imshow(\"result\", img)\n",
        "    cv2_imshow(img)\n",
        "    key = cv2.waitKey(-1) if image_dir else cv2.waitKey(30)\n",
        "\n",
        "    if key == 27:  # ESC\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlMjaOxSbXae"
      },
      "source": [
        "INTERFACING WITH OLD CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1-acT1VOC1K",
        "outputId": "1f030d92-ef26-4a23-808a-3f694578ea26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from utils import *\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "from contextlib import contextmanager\n",
        "from wide_resnet import WideResNet\n",
        "from keras.utils.data_utils import get_file\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
        "               font_scale=1.0, thickness=3):\n",
        "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
        "    x, y = point\n",
        "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
        "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozAdxenmaiYg"
      },
      "source": [
        "def iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    box1 -- first box, list object with coordinates (box1_x1, box1_y1, box1_x2, box_1_y2)\n",
        "    box2 -- second box, list object with coordinates (box2_x1, box2_y1, box2_x2, box2_y2)\n",
        "    \"\"\"\n",
        "    # Assign variable names to coordinates for clarity\n",
        "    (box1_x1, box1_y1, box1_x2, box1_y2) = box1\n",
        "    (box2_x1, box2_y1, box2_x2, box2_y2) = box2\n",
        "    \n",
        "    # Calculate the (yi1, xi1, yi2, xi2) coordinates of the intersection of box1 and box2. Calculate its Area.\n",
        "    ### START CODE HERE ### (≈ 7 lines)\n",
        "    xi1 = max(box1_x1,box2_x1)\n",
        "    yi1 = max(box1_y1,box2_y1)\n",
        "    xi2 = min(box1_x2,box2_x2)\n",
        "    yi2 = min(box1_y2,box2_y2)\n",
        "    inter_width = xi2-xi1\n",
        "    inter_height = yi2-yi1\n",
        "    inter_area = max(inter_width,0) * max(inter_height,0)\n",
        "    ### END CODE HERE ###    \n",
        "\n",
        "    # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)\n",
        "    ### START CODE HERE ### (≈ 3 lines)\n",
        "    box1_area = (box1_x2-box1_x1) * (box1_y2-box1_y1)\n",
        "    box2_area = (box2_x2-box2_x1) * (box2_y2-box2_y1)\n",
        "    union_area = (box1_area + box2_area) - inter_area\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # compute the IoU\n",
        "    ### START CODE HERE ### (≈ 1 line)\n",
        "    iou = inter_area / union_area\n",
        "    ### END CODE HERE ###\n",
        "    return iou\n",
        "class Sort(object):\n",
        "  def __init__(self, max_age=1, iou_thresh =0.4):\n",
        "    \"\"\"\n",
        "    Sets key parameters for SORT\n",
        "    \"\"\"\n",
        "    #Age denotes the expiring of a track\n",
        "    self.max_age = max_age\n",
        "    self.iou_thresh = iou_thresh\n",
        "    self.p_track = []\n",
        "    self.frame_count = 0\n",
        "    self.id = 0\n",
        "    \n",
        "  def update(self, dets=np.empty((0, 5))):\n",
        "    # print(dets)\n",
        "    self.frame_count += 1\n",
        "    # self.p_track[:]['age'] += 1\n",
        "    updated_trk = []\n",
        "    # if self.id==0 and dets.size != 0:\n",
        "    if not self.p_track and dets.size != 0:\n",
        "      #Initialise first detections as Previous Trackers & if not first add with previous id(When p_track becomes empty)\n",
        "      self.p_track = [{'bbox':list(det),'id':k + self.id,'age':0,'frame_count':1} for k,det in enumerate(dets,1)]\n",
        "      self.id = self.p_track[-1]['id']\n",
        "    elif self.p_track and dets.size != 0:\n",
        "      # print(\"####\")\n",
        "      # print(self.p_track)\n",
        "      new_p = self.p_track\n",
        "      for det in dets:\n",
        "        try:\n",
        "          best_match_trk = max(new_p,key=lambda x:iou(det,x['bbox']))\n",
        "        except ValueError:\n",
        "          print(\"################################### VALUE ERROR ####################################################\\n\"+\"det: \",end=\" \")\n",
        "          print(det)\n",
        "          print(\"prev trk: \",end=\" \")\n",
        "          print(self.p_track)\n",
        "          continue\n",
        "        # print(iou(det,best_match_trk['bbox']))\n",
        "        if iou(det,best_match_trk['bbox']) >= self.iou_thresh :\n",
        "          best_match_trk['bbox'] = det\n",
        "          best_match_trk['frame_count'] += 1\n",
        "          # print(best_match_trk)\n",
        "          #reset age\n",
        "          best_match_trk['age'] = 0\n",
        "          updated_trk.append(best_match_trk)\n",
        "        else:\n",
        "          #New non matched detection is assigned to new track\n",
        "          self.id += 1\n",
        "          new_trk = {'bbox':det,'id':self.id,'age':0,'frame_count':1}\n",
        "          updated_trk.append(new_trk)\n",
        "          if best_match_trk['age'] <= self.max_age:\n",
        "            best_match_trk['age'] += 1\n",
        "            updated_trk.append(best_match_trk)\n",
        "        #removing the used best_match_trk from p_track\n",
        "        # del dets[dets.index(best_match)]\n",
        "        # print(self.p_track[self.p_track.index(best_match_trk)])\n",
        "        # print(\"before:\")\n",
        "        # print(self.p_track)\n",
        "        # print(best_match_trk)\n",
        "        # print(self.p_track.index(best_match_trk))\n",
        "        # del self.p_track[self.p_track.index(best_match_trk)]\n",
        "        # self.p_track.remove(best_match_trk)\n",
        "        self.p_track = list(filter(lambda x: x['id'] != best_match_trk['id'] and x['age'] < self.max_age, self.p_track)) \n",
        "        # print(\"after:\")\n",
        "        # print(self.p_track)        \n",
        "      #increase age of previous tracks\n",
        "      for trk in self.p_track:\n",
        "        trk['age'] += 1\n",
        "      self.p_track = self.p_track + updated_trk\n",
        "    elif self.p_track and dets.size == 0:\n",
        "      new_p = []\n",
        "      for trk in self.p_track:\n",
        "        trk['age'] += 1\n",
        "        if(trk['age'] < self.max_age):\n",
        "          new_p.append(trk)\n",
        "      self.p_track = new_p\n",
        "      return []\n",
        "        \n",
        "      \n",
        "    # print(\"#######\",end = \" \")\n",
        "    # print(self.p_track,end=\"#######\")\n",
        "    return self.p_track\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqf_aiPbb0rl",
        "outputId": "d53c07f4-bdca-4899-eb2a-d4de6d17fd82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "model_cfg = './cfg/yolov3-face.cfg'\n",
        "model_weights = './model-weights/yolov3-wider_16000.weights'\n",
        "# image = './samples/outside_000001.jpg'\n",
        "video = './samples/sss.mp4'\n",
        "# output_dir = '/content/drive/My Drive/yoloface/outputs/'\n",
        "output_dir = '/content/age-gender-estimation/outputs/'\n",
        "\n",
        "IOU_THRESH = 0.35\n",
        "MAX_AGE = 3\n",
        "#Trackers(Here faces) with given frame count value is chosen for age and gender processing\n",
        "FRAME_COUNT_THRESH = 6\n",
        "# DETECT_INTERVAL = 2\n",
        "FACE_HEIGHT = 50\n",
        "FACE_WIDTH = 50\n",
        "# print the arguments\n",
        "print('----- info -----')\n",
        "print('[i] The config file: ',model_cfg)\n",
        "print('[i] The weights of model file: ',model_weights)\n",
        "# print('[i] Path to image file: ',image)\n",
        "print('[i] Path to video file: ',video)\n",
        "print('[i] IOU Threshold value: ',IOU_THRESH)\n",
        "print('[i] Max age value: ',MAX_AGE)\n",
        "print('[i] Frame count threshold value: ',FRAME_COUNT_THRESH)\n",
        "# print('[i] Detect interval: ',DETECT_INTERVAL)\n",
        "print('[i] Minimum face height required: ',FACE_HEIGHT)\n",
        "print('[i] Minimum face width required: ',FACE_WIDTH)\n",
        "\n",
        "\n",
        "#Read config net\n",
        "net = cv2.dnn.readNetFromDarknet(model_cfg,model_weights)\n",
        "print('###########################################################\\n')\n",
        "\n",
        "\n",
        "#VIDEO\n",
        "cap = cv2.VideoCapture(video)\n",
        "output_file = video[:-4].rsplit('/')[-1] + '_keras.avi'\n",
        "output_file = output_dir + output_file\n",
        "\n",
        "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
        "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
        "\n",
        "video_writer = cv2.VideoWriter(output_file,cv2.VideoWriter_fourcc(*'XVID'),cap.get(cv2.CAP_PROP_FPS), (\n",
        "                                          round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "                                          round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
        "\n",
        "\n",
        "# print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "# print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "print(cap.get(cv2.CAP_PROP_FPS))\n",
        "#create instance of SORT\n",
        "mot = Sort(MAX_AGE,IOU_THRESH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- info -----\n",
            "[i] The config file:  ./cfg/yolov3-face.cfg\n",
            "[i] The weights of model file:  ./model-weights/yolov3-wider_16000.weights\n",
            "[i] Path to video file:  ./samples/sss.mp4\n",
            "[i] IOU Threshold value:  0.35\n",
            "[i] Max age value:  3\n",
            "[i] Frame count threshold value:  6\n",
            "[i] Minimum face height required:  50\n",
            "[i] Minimum face width required:  50\n",
            "###########################################################\n",
            "\n",
            "23.976023976023978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7URCCQTTd4M4"
      },
      "source": [
        "depth = 16\n",
        "k = 8\n",
        "weight_file = \"/content/age-gender-estimation/pretrained_models/weights.28-3.73.hdf5\"\n",
        "margin = 0.4\n",
        "image_dir = None\n",
        "# load model and weights\n",
        "img_size = 64\n",
        "# img_size = 32\n",
        "model = WideResNet(img_size, depth=depth, k=k)()\n",
        "model.load_weights(weight_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb7dGPz0T2G6"
      },
      "source": [
        "#Frame counter for detect interval\n",
        "# c=0\n",
        "while True:\n",
        "  \n",
        "  has_frame, frame = cap.read()\n",
        "  if not has_frame:\n",
        "      print('[i] ==> Done processing!!!')\n",
        "      #print('[i] ==> Output file is stored at', os.path.join(args.output_dir, output_file))\n",
        "      break\n",
        "  # frame = cv2.resize(frame, (0, 0), fx=0.7, fy=0.7)\n",
        "  # Resize, Convert BGR to HSV\n",
        "  # if ((IMG_HEIGHT, IMG_WIDTH) != frame.shape[0:2]):\n",
        "  #     frame = cv2.resize(frame, dsize=(IMG_WIDTH, IMG_HEIGHT), fx=0, fy=0)\n",
        "  # else:\n",
        "  #     frame = frame\n",
        "  # if(c % DETECT_INTERVAL==0):\n",
        "  new_frame = frame.copy()\n",
        "  img_h, img_w, _ = np.shape(frame)\n",
        "\n",
        "  blob = cv2.dnn.blobFromImage(frame, 1 / 255, (IMG_WIDTH, IMG_HEIGHT),[0, 0, 0], 1, crop=False)\n",
        "  net.setInput(blob)\n",
        "  # Runs the forward pass to get output of the output layers\n",
        "  outs = net.forward(get_outputs_names(net))\n",
        "  # Remove the bounding boxes with low confidence\n",
        "  faces = post_process(frame, outs,CONF_THRESHOLD, NMS_THRESHOLD)\n",
        "  faces = np.array(faces) \n",
        "  \n",
        "  # faces_resized = []\n",
        "\n",
        "  if(faces.shape[0]>0):\n",
        "    final_faces = faces[:,:4]\n",
        "    \n",
        "  else:\n",
        "    final_faces = faces\n",
        "  trackers = mot.update(final_faces)\n",
        "  # print(\"Final faces:\",end=\" \")\n",
        "  # print(final_faces)\n",
        "  # print(\"$$$$$$$$$$$$$ \") \n",
        "  # print(trackers)\n",
        "  # print(\"&&&&&&&&&&&&&&&&&&\")\n",
        "  print(\"#\",end=\"\")\n",
        "  faces_resized = np.empty((len(trackers), img_size, img_size, 3))\n",
        "\n",
        "  for i,trk in enumerate(trackers):\n",
        "    box = np.array(trk['bbox'])\n",
        "    id = int(trk['id'])\n",
        "    frame_count = trk['frame_count']\n",
        "    d = box.astype(np.int32)\n",
        "    (startX, startY, endX, endY) = (d[0],d[1],d[2],d[3])\n",
        "    face_h = endY - startY\n",
        "    face_w = endX - startX\n",
        "\n",
        "    x1, y1, x2, y2, w, h = d[0], d[1], d[2] + 1, d[3] + 1, face_w, face_h\n",
        "    xw1 = max(int(x1 - margin * w), 0)\n",
        "    yw1 = max(int(y1 - margin * h), 0)\n",
        "    xw2 = min(int(x2 + margin * w), img_w - 1)\n",
        "    yw2 = min(int(y2 + margin * h), img_h - 1)\n",
        "    # cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "    cv2.rectangle(frame, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\n",
        "    # faces[i, :, :, :] = cv2.resize(img[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # if(frame_count==FRAME_COUNT_THRESH and (face_h > FACE_HEIGHT and face_w > FACE_WIDTH)):\n",
        "    # crop_face = new_frame[startY:endY, startX:endX]\n",
        "    crop_face_shape = new_frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
        "\n",
        "    crop_face = cv2.resize(new_frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
        "    # crop_face = cv2.resize( new_frame[startY:endY, startX:endX], (img_size,img_size))\n",
        "\n",
        "    print(crop_face_shape.shape)\n",
        "\n",
        "\n",
        "\n",
        "    faces_resized[i,:,:,:] = crop_face\n",
        "\n",
        "    if(frame_count == FRAME_COUNT_THRESH):  \n",
        "      try:\n",
        "        cv2.imwrite(\"{0}/{1}.jpg\".format(\"facepics\",id),crop_face)\n",
        "        # cv2_imshow(crop_face)\n",
        "      except:\n",
        "        print(\"#########################Image error####################\")\n",
        "        continue\n",
        "\n",
        "\n",
        "  # predict ages and genders of the detected faces\n",
        "  # print(faces_resized)\n",
        "\n",
        "\n",
        "  if(faces_resized.size > 0):\n",
        "    results = model.predict(faces_resized)\n",
        "    predicted_genders = results[0]\n",
        "    ages = np.arange(0, 101).reshape(101, 1)\n",
        "    predicted_ages = results[1].dot(ages).flatten()\n",
        "    for i,trk in enumerate(trackers):\n",
        "      # if(frame_count==FRAME_COUNT_THRESH and (face_h > FACE_HEIGHT and face_w > FACE_WIDTH)):\n",
        "      d = np.array(trk['bbox']).astype(np.int32)\n",
        "      id = int(trk['id'])\n",
        "      label = \"{}, {}, {}\".format(id,int(predicted_ages[i]),\"M\" if predicted_genders[i][0] < 0.5 else \"F\")\n",
        "      draw_label(frame, (d[0], d[1]), label)\n",
        "\n",
        "\n",
        "\n",
        "    # print(results)\n",
        "    # print(\"#\" * 20)\n",
        "    # print(predicted_genders)\n",
        "    # print(\"#\" * 20)\n",
        "    # print(ages)\n",
        "    # print(\"#\" * 20)\n",
        "    # print(predicted_ages)\n",
        "    # print(\"#\" * 20)\n",
        "    # draw results\n",
        "\n",
        "\n",
        "\n",
        "  # cv2_imshow(frame)\n",
        "  video_writer.write(frame.astype(np.uint8))\n",
        "  # c+=1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_8XNVXboIyV"
      },
      "source": [
        "!zip -r /content/file.zip /content/age-gender-estimation/facepics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmGPuhvpoTxi",
        "outputId": "e9bc1af3-ad05-4026-cbd3-db7436b55526",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_09a82bf5-5638-4689-90ac-32c1c16aade1\", \"file.zip\", 13068)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A7r2pUTwlV1"
      },
      "source": [
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHIl1b13r9nj"
      },
      "source": [
        "cap.release()\n",
        "video_writer.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC8sGKgJPsTo"
      },
      "source": [
        "new_out = \"/content/drive/My Drive/yoloface/facepics/\" + video[:-4].rsplit('/')[-1] + '_pics'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6W969AIRG1q",
        "outputId": "f53553ab-0e8e-412a-e065-75efd46b442f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(new_out)\n",
        "print(output_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/yoloface/facepics/pro_sample_pics\n",
            "/content/age-gender-estimation/outputs/pro_sample_keras.avi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26IQ0MyH2zcP"
      },
      "source": [
        "!cp \"facepics\" \"/content/drive/My Drive/yoloface/facepics/sushant_pics\" -r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yemVMotFP2mH"
      },
      "source": [
        "!cp \"/content/age-gender-estimation/outputs/sushant_keras.avi\" \"/content/drive/My Drive/yoloface/outputs/sushant_keras.avi\" -r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4qcdK3PQgFM"
      },
      "source": [
        "rm facepics -r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axgv9r2JQjSO"
      },
      "source": [
        "mkdir \"facepics\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1T73NozQovB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}